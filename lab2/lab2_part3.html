<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>lab2_part3 - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html","displayName":"Databricks Guide","icon":"question"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html","displayName":"Application Examples","icon":"code"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/courses/index.html","displayName":"Training","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableInstanceProfilesUIInJobs":false,"nodeInfo":{"node_types":[{"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"memory_mb":6144,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false}],"default_node_type_id":"dev-tier-node"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":false,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Scala 2.11)","packageLabel":"spark-image-ed3159750ab6758ce69604f1f21d763e28c29759ea3bfd4d5bc40fbdb781442d","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Scala 2.10)","packageLabel":"spark-image-e0993f7184eb1ed91009cbd35545d8359b2ff173578876088427f50ca20f9fd0","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":8,"memory-optimized":1,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.29.1","accountsLimit":3,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"disableLegacyDashboards":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"useFixedStaticNotebookVersionForDevelopment":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAclService":true,"docsDomain":"https://docs.cloud.databricks.com/","enableWorkspaceAcls":false,"gitHash":"04f6f58f56212023faa4031e2ec1f5d4aa018fc7","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"showDevTierBetaVersion":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/A%20Gentle%20Introduction%20to%20Apache%20Spark%20on%20Databricks.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/Quick%20Start%20DataFrames.html","displayName":"Quick Start DataFrames","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/GSW%20Passing%20Analysis%20(new).html","displayName":"GSW Passing Analysis (new)","icon":"img/home/Python_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"enableTerminal":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":true,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":183330036760970,"name":"lab2_part3","language":"scala","commands":[{"version":"CommandV1","origId":183330036760972,"guid":"ce051106-ceba-47d5-9fce-df432d6edc81","subtype":"command","commandType":"auto","position":1.0,"command":"val AccessKey = \"AKIAJMYWD5JGXJEFWAKQ\"\nval SecretKey = \"002bcl6Ebijr7E8Tbg9rMJX+0fwMnWiGlSAXbuRV\"\nval EncodedSecretKey = SecretKey.replace(\"/\", \"%2F\")\nval AwsBucketName = \"data-intensive-course\"\nval MountName = \"apacheLog2\"\ndbutils.fs.mount(s\"s3a://$AccessKey:$EncodedSecretKey@$AwsBucketName\", s\"/mnt/$MountName\")\ndisplay(dbutils.fs.ls(s\"/mnt/$MountName\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/mnt/apacheLog2/apache.log","apache.log",5.0354303E7],["dbfs:/mnt/apacheLog2/part-00053","part-00053",3.549592E7]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\""},{"name":"name","type":"\"string\""},{"name":"size","type":"\"long\""}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/apacheLog1; nested exception is: ","error":"<div class=\"ansiout\">\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/apacheLog1\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:73)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:42)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:315)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.mount(DbfsUtilsImpl.scala:77)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/apacheLog1\n\tat scala.Predef$.require(Predef.scala:233)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:121)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:38)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:58)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:57)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:57)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:213)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:194)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:41)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:36)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:56)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:56)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$10.apply(JettyServer.scala:263)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:263)\n\tat com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:200)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:145)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:136)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:136)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:145)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:140)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:75)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:178)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:75)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:135)\n\tat com.databricks.rpc.JettyServer$RequestManager.doGet(JettyServer.scala:90)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:845)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:524)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:319)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:253)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\tat java.lang.Thread.run(Thread.java:745)</div>","workflows":[],"startTime":1.475254704868E12,"submitTime":1.475254704385E12,"finishTime":1.475254710605E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"50c29e69-3ecb-4105-b528-1872faaba26e"},{"version":"CommandV1","origId":183330036760973,"guid":"4f694790-9800-4009-8cb4-580ae4b720a7","subtype":"command","commandType":"auto","position":2.0,"command":"// Just run this code\nimport scala.util.matching\nimport org.apache.spark.rdd.RDD\n\ncase class Cal(year: Int, month: Int, day: Int, hour: Int, minute: Int, second: Int)                \n\ncase class Row(host: String,\n               client_identd: String,\n               user_id: String,\n               date_time: Cal,\n               method: String,\n               endpoint: String,\n               protocol: String,\n               response_code: Int,\n               content_size: Long)\n                \n\nval month_map = Map(\"Jan\" -> 1, \"Feb\" -> 2, \n                    \"Mar\" -> 3, \"Apr\" -> 4, \n                    \"May\" -> 5, \"Jun\" -> 6, \n                    \"Jul\" -> 7, \"Aug\" -> 8,  \n                    \"Sep\" -> 9, \"Oct\" -> 10, \n                    \"Nov\" -> 11, \"Dec\" -> 12)\n\ndef parse_apache_time(s: String): Cal = {\n    // Convert Apache time format into a Python datetime object\n    // Args:\n    //    s (str): date and time in Apache time format\n    // Returns:\n    //   datetime: datetime object (ignore timezone for now)\n\n    return Cal(s.substring(7, 11).toInt,\n            month_map(s.substring(3, 6)),\n            s.substring(0, 2).toInt,\n            s.substring(12, 14).toInt,\n            s.substring(15, 17).toInt,\n            s.substring(18, 20).toInt)\n}\n\ndef parseApacheLogLine(logline: String): (Either[Row, String], Int) = {\n    // Parse a line in the Apache Common Log format\n    // Args:\n    //    logline (str): a line of text in the Apache Common Log format\n    // Returns:\n    //    tuple: either a dictionary containing the parts of the Apache Access Log and 1,\n    //           or the original invalid log line and 0\n    \n    val ret = APACHE_ACCESS_LOG_PATTERN.findAllIn(logline).matchData.toList\n    if (ret.isEmpty)\n        return (Right(logline), 0)\n\n    val r = ret(0)\n    val size_field = r.group(9)\n\n    var size: Long = 0\n    if (size_field != \"-\")\n        size = size_field.toLong\n\n    return (Left(Row(\n            r.group(1),\n            r.group(2),\n            r.group(3),\n            parse_apache_time(r.group(4)),\n            r.group(5),\n            r.group(6),\n            r.group(7),\n            r.group(8).toInt,\n            size)), 1)\n}\n\ndef parseLogs(): (RDD[(Either[Row, String], Int)], RDD[Row], RDD[String]) = {\n    val fileName = s\"/mnt/$MountName/apache.log\"\n    \n    val parsed_logs = sc.textFile(fileName).map(parseApacheLogLine).cache()\n    val access_logs = parsed_logs.filter(x => x._2 == 1).map(x => x._1.left.get)\n    val failed_logs = parsed_logs.filter(x => x._2 == 0).map(x => x._1.right.get)\n\n    val failed_logs_count = failed_logs.count()\n    \n    if (failed_logs_count > 0) {\n        println(s\"Number of invalid logline: $failed_logs.count()\")\n        failed_logs.take(20).foreach(println)\n    }\n    \n    println(s\"Read $parsed_logs.count() lines, successfully parsed $access_logs.count() lines, and failed to parse $failed_logs.count()\")\n    \n    return (parsed_logs, access_logs, failed_logs)\n}\n\n// A regular expression pattern to extract fields from the log line\nval APACHE_ACCESS_LOG_PATTERN = \"\"\"^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\\s*\" (\\d{3}) (\\S+)\"\"\".r\n\nval (parsed_logs, access_logs, failed_logs) = parseLogs()\naccess_logs.take(20).foreach(println)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Read MapPartitionsRDD[59] at map at &lt;console&gt;:107.count() lines, successfully parsed MapPartitionsRDD[61] at map at &lt;console&gt;:108.count() lines, and failed to parse MapPartitionsRDD[63] at map at &lt;console&gt;:109.count()\nRow(in24.inetnebr.com,-,-,Cal(1995,8,1,0,0,1),GET,/shuttle/missions/sts-68/news/sts-68-mcc-05.txt,HTTP/1.0,200,1839)\nRow(uplherc.upl.com,-,-,Cal(1995,8,1,0,0,7),GET,/,HTTP/1.0,304,0)\nRow(uplherc.upl.com,-,-,Cal(1995,8,1,0,0,8),GET,/images/ksclogo-medium.gif,HTTP/1.0,304,0)\nRow(uplherc.upl.com,-,-,Cal(1995,8,1,0,0,8),GET,/images/MOSAIC-logosmall.gif,HTTP/1.0,304,0)\nRow(uplherc.upl.com,-,-,Cal(1995,8,1,0,0,8),GET,/images/USA-logosmall.gif,HTTP/1.0,304,0)\nRow(ix-esc-ca2-07.ix.netcom.com,-,-,Cal(1995,8,1,0,0,9),GET,/images/launch-logo.gif,HTTP/1.0,200,1713)\nRow(uplherc.upl.com,-,-,Cal(1995,8,1,0,0,10),GET,/images/WORLD-logosmall.gif,HTTP/1.0,304,0)\nRow(slppp6.intermind.net,-,-,Cal(1995,8,1,0,0,10),GET,/history/skylab/skylab.html,HTTP/1.0,200,1687)\nRow(piweba4y.prodigy.com,-,-,Cal(1995,8,1,0,0,10),GET,/images/launchmedium.gif,HTTP/1.0,200,11853)\nRow(slppp6.intermind.net,-,-,Cal(1995,8,1,0,0,11),GET,/history/skylab/skylab-small.gif,HTTP/1.0,200,9202)\nRow(slppp6.intermind.net,-,-,Cal(1995,8,1,0,0,12),GET,/images/ksclogosmall.gif,HTTP/1.0,200,3635)\nRow(ix-esc-ca2-07.ix.netcom.com,-,-,Cal(1995,8,1,0,0,12),GET,/history/apollo/images/apollo-logo1.gif,HTTP/1.0,200,1173)\nRow(slppp6.intermind.net,-,-,Cal(1995,8,1,0,0,13),GET,/history/apollo/images/apollo-logo.gif,HTTP/1.0,200,3047)\nRow(uplherc.upl.com,-,-,Cal(1995,8,1,0,0,14),GET,/images/NASA-logosmall.gif,HTTP/1.0,304,0)\nRow(133.43.96.45,-,-,Cal(1995,8,1,0,0,16),GET,/shuttle/missions/sts-69/mission-sts-69.html,HTTP/1.0,200,10566)\nRow(kgtyk4.kj.yamagata-u.ac.jp,-,-,Cal(1995,8,1,0,0,17),GET,/,HTTP/1.0,200,7280)\nRow(kgtyk4.kj.yamagata-u.ac.jp,-,-,Cal(1995,8,1,0,0,18),GET,/images/ksclogo-medium.gif,HTTP/1.0,200,5866)\nRow(d0ucr6.fnal.gov,-,-,Cal(1995,8,1,0,0,19),GET,/history/apollo/apollo-16/apollo-16.html,HTTP/1.0,200,2743)\nRow(ix-esc-ca2-07.ix.netcom.com,-,-,Cal(1995,8,1,0,0,19),GET,/shuttle/resources/orbiters/discovery.html,HTTP/1.0,200,6849)\nRow(d0ucr6.fnal.gov,-,-,Cal(1995,8,1,0,0,20),GET,/history/apollo/apollo-16/apollo-16-patch-small.gif,HTTP/1.0,200,14897)\nimport scala.util.matching\nimport org.apache.spark.rdd.RDD\ndefined class Cal\ndefined class Row\nmonth_map: scala.collection.immutable.Map[String,Int] = Map(Nov -&gt; 11, Jul -&gt; 7, Mar -&gt; 3, Jan -&gt; 1, Oct -&gt; 10, Dec -&gt; 12, Feb -&gt; 2, May -&gt; 5, Apr -&gt; 4, Aug -&gt; 8, Sep -&gt; 9, Jun -&gt; 6)\nparse_apache_time: (s: String)Cal\nparseApacheLogLine: (logline: String)(Either[Row,String], Int)\nparseLogs: ()(org.apache.spark.rdd.RDD[(Either[Row,String], Int)], org.apache.spark.rdd.RDD[Row], org.apache.spark.rdd.RDD[String])\nAPACHE_ACCESS_LOG_PATTERN: scala.util.matching.Regex = ^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] &quot;(\\S+) (\\S+)\\s*(\\S*)\\s*&quot; (\\d{3}) (\\S+)\nparsed_logs: org.apache.spark.rdd.RDD[(Either[Row,String], Int)] = MapPartitionsRDD[59] at map at &lt;console&gt;:107\naccess_logs: org.apache.spark.rdd.RDD[Row] = MapPartitionsRDD[61] at map at &lt;console&gt;:108\nfailed_logs: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[63] at map at &lt;console&gt;:109\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:103: error: not found: value MountName\n    val fileName = s&quot;/mnt/$MountName/apache.log&quot;\n                           ^\n</div>","error":null,"workflows":[],"startTime":1.475254715496E12,"submitTime":1.475254715012E12,"finishTime":1.475254728593E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"bfcc70bb-d989-4189-8807-513fca0c5f52"},{"version":"CommandV1","origId":183330036760974,"guid":"919a66bb-0cfe-492f-9237-c100472e8729","subtype":"command","commandType":"auto","position":3.0,"command":"// Calculate statistics based on the content size.\nval content_sizes = access_logs.map(log => log.content_size).cache()\n\nprintln(\"Content Size Avg: \" + (content_sizes.reduce(_+_)/ content_sizes.count()) +\n         \", Min: \" + content_sizes.min() +\n         \", Max: \" + content_sizes.max())","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Content Size Avg: 17528, Min: 0, Max: 3421948\ncontent_sizes: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[393] at map at &lt;console&gt;:37\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:41: error: wrong number of parameters; expected = 1\n              println(&quot;Content Size Avg: &quot; + (content_sizes.map((a,b)=&gt;a+b)/ content_sizes.count()) +\n                                                                     ^\n</div>","error":null,"workflows":[],"startTime":1.47518238501E12,"submitTime":1.475182384822E12,"finishTime":1.475182385435E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"382c4c9c-1690-43c7-9524-561676e69d80"},{"version":"CommandV1","origId":183330036760975,"guid":"31eb1e5b-86ac-431f-8ce3-023c1eba42a0","subtype":"command","commandType":"auto","position":4.0,"command":"val responseCodeToCount = access_logs.map(log => (log.response_code, 1)).reduceByKey(_+_).cache()\n\nval responseCodeToCountList = responseCodeToCount.take(100)\n\nprintln(\"Found \" + responseCodeToCountList.length + \" response codes\")\nprint(\"Response Code Counts: \")\nresponseCodeToCountList.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Found 7 response codes\nResponse Code Counts: (404,3137) (200,426183) (302,6994) (304,33660) (500,2) (403,21) (501,3) responseCodeToCount: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[432] at reduceByKey at &lt;console&gt;:34\nresponseCodeToCountList: Array[(Int, Int)] = Array((404,3137), (200,426183), (302,6994), (304,33660), (500,2), (403,21), (501,3))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:3: error: ')' expected but double literal found.\n       val responseCodeToCount = access_logs.map(log =&gt; (log.response_code, 1)).reduceByKey(_.2+_.2).cache()\n                                                                                             ^\n&lt;console&gt;:9: error: ';' expected but ')' found.\n       responseCodeToCountList.foreach(x =&gt; print(x + &quot; &quot;))\n                                                          ^\n</div>","error":null,"workflows":[],"startTime":1.475182945439E12,"submitTime":1.475182945236E12,"finishTime":1.475182946316E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"064748b9-af8b-4db2-b591-9391d6199bdd"},{"version":"CommandV1","origId":183330036760976,"guid":"9460bc9f-9c15-4063-9b5b-93a5f5865d8a","subtype":"command","commandType":"auto","position":5.0,"command":"// Any hosts that has accessed the server more than 10 times.\nval hostCountPairTuple = access_logs.map(log => (log.host, 1))\nval hostSum = hostCountPairTuple.reduceByKey(_+_)\nval hostMoreThan10 = hostSum.filter(_._2>=10)\nval hostsPick20 = hostMoreThan10.map(x => x._1).take(20)\n\nprint(\"Any 20 hosts that have accessed more then 10 times: \")\nhostsPick20.foreach(x => println(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Any 20 hosts that have accessed more then 10 times: n1043347.ksc.nasa.gov \n193.74.242.28 \nd02.as1.nisiq.net \njcday.nccts.drenet.dnd.ca \nip-pdx2-56.teleport.com \n192.112.22.82 \nts6-5.slip.uwo.ca \nanx3p4.trib.com \n198.77.113.34 \n204.235.86.107 \ns9.its.bldrdoc.gov \ncrc182.cac.washington.edu \n204.255.92.30 \n161.243.222.10 \ntelford-107.salford.ac.uk \nip-vanc2-10.teleport.com \nuniverse6.barint.on.ca \ngatekeeper.homecare.com \nkaifmv.tksc.nasda.go.jp \nunknown.edsa.co.za \nhostCountPairTuple: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[494] at map at &lt;console&gt;:43\nhostSum: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[495] at reduceByKey at &lt;console&gt;:44\nhostMoreThan10: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[496] at filter at &lt;console&gt;:45\nhostsPick20: Array[String] = Array(n1043347.ksc.nasa.gov, 193.74.242.28, d02.as1.nisiq.net, jcday.nccts.drenet.dnd.ca, ip-pdx2-56.teleport.com, 192.112.22.82, ts6-5.slip.uwo.ca, anx3p4.trib.com, 198.77.113.34, 204.235.86.107, s9.its.bldrdoc.gov, crc182.cac.washington.edu, 204.255.92.30, 161.243.222.10, telford-107.salford.ac.uk, ip-vanc2-10.teleport.com, universe6.barint.on.ca, gatekeeper.homecare.com, kaifmv.tksc.nasda.go.jp, unknown.edsa.co.za)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:35: error: value hostname is not a member of Row\n              val hostCountPairTuple = access_logs.map(log =&gt; (log.hostname, 1))\n                                                                   ^\n&lt;console&gt;:36: error: value reduceByKey is not a member of org.apache.spark.rdd.RDD[Nothing]\n              val hostSum = hostCountPairTuple.reduceByKey(_+_)\n                                               ^\n</div>","error":null,"workflows":[],"startTime":1.47518330513E12,"submitTime":1.475183304925E12,"finishTime":1.475183306276E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"093a4905-b37b-402e-a439-8093cb4cf8a5"},{"version":"CommandV1","origId":183330036760977,"guid":"b492c621-2e8b-4e19-bba9-0054c57c67e8","subtype":"command","commandType":"auto","position":6.0,"command":"// TODO: Replace <FILL IN> with appropriate code\n// Top Endpoints\nval endpointCounts = access_logs.map(log=>(log.endpoint,1)).reduceByKey(_+_)\nval topEndpoints = endpointCounts.takeOrdered(10)(Ordering[Int].on(x=>(-x._2)))\n\nprint(\"Top Ten Endpoints: \")\ntopEndpoints.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Top Ten Endpoints: (/images/NASA-logosmall.gif,27035) (/images/KSC-logosmall.gif,21458) (/images/MOSAIC-logosmall.gif,20254) (/images/USA-logosmall.gif,20172) (/images/WORLD-logosmall.gif,20004) (/images/ksclogo-medium.gif,19300) (/ksc.html,13508) (/history/apollo/images/apollo-logo1.gif,11074) (/images/launch-logo.gif,10120) (/,9481) endpointCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[122] at reduceByKey at &lt;console&gt;:40\ntopEndpoints: Array[(String, Int)] = Array((/images/NASA-logosmall.gif,27035), (/images/KSC-logosmall.gif,21458), (/images/MOSAIC-logosmall.gif,20254), (/images/USA-logosmall.gif,20172), (/images/WORLD-logosmall.gif,20004), (/images/ksclogo-medium.gif,19300), (/ksc.html,13508), (/history/apollo/images/apollo-logo1.gif,11074), (/images/launch-logo.gif,10120), (/,9481))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:41: error: not found: value x\n              val topEndpoints = endpointCounts.takeOrdered(10)(Ordering[Int].on(x=&gt;-x._2))\n                                                                                 ^\n</div>","error":null,"workflows":[],"startTime":1.475215257017E12,"submitTime":1.475215256847E12,"finishTime":1.475215257633E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"f15b9326-29d5-47e7-b5f3-7772cadd27ef"},{"version":"CommandV1","origId":1164226758675456,"guid":"60f3b630-039b-4927-aca4-c0d8490c7f7a","subtype":"command","commandType":"auto","position":7.0,"command":"// TODO: Replace <FILL IN> with appropriate code\n\nval not200 = access_logs.filter(log => log.response_code != 200)\nval endpointCountPairTuple = not200.map(log => (log.endpoint, 1))\nval endpointSum = endpointCountPairTuple.reduceByKey(_+_)\nval topTenErrURLs = endpointSum.takeOrdered(10)(Ordering[Int].on(x=>(-x._2)))\n\nprint(\"Top Ten failed URLs: \")\ntopTenErrURLs.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Top Ten failed URLs: (/images/NASA-logosmall.gif,3914) (/images/KSC-logosmall.gif,2906) (/images/MOSAIC-logosmall.gif,2107) (/images/USA-logosmall.gif,2079) (/images/WORLD-logosmall.gif,2006) (/images/ksclogo-medium.gif,1929) (/history/apollo/images/apollo-logo1.gif,1067) (/images/launch-logo.gif,1022) (/,871) (/images/ksclogosmall.gif,663) not200: org.apache.spark.rdd.RDD[Row] = MapPartitionsRDD[67] at filter at &lt;console&gt;:36\nendpointCountPairTuple: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[68] at map at &lt;console&gt;:37\nendpointSum: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[69] at reduceByKey at &lt;console&gt;:38\ntopTenErrURLs: Array[(String, Int)] = Array((/images/NASA-logosmall.gif,3914), (/images/KSC-logosmall.gif,2906), (/images/MOSAIC-logosmall.gif,2107), (/images/USA-logosmall.gif,2079), (/images/WORLD-logosmall.gif,2006), (/images/ksclogo-medium.gif,1929), (/history/apollo/images/apollo-logo1.gif,1067), (/images/launch-logo.gif,1022), (/,871), (/images/ksclogosmall.gif,663))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.475254731008E12,"submitTime":1.475254730526E12,"finishTime":1.475254731971E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"d36f7bbb-e6e7-4499-be10-6cf21a97b875"},{"version":"CommandV1","origId":137283740762872,"guid":"6212905f-7fad-49cb-b78d-f33ec8490139","subtype":"command","commandType":"auto","position":8.0,"command":"val hosts = access_logs.map(log => log.host)\nval uniqueHosts = hosts.distinct\nval uniqueHostCount = uniqueHosts.count()\nprintln(\"Unique hosts: \" + uniqueHostCount)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Unique hosts: 26672\nhosts: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[97] at map at &lt;console&gt;:34\nuniqueHosts: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[100] at distinct at &lt;console&gt;:35\nuniqueHostCount: Long = 26672\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:34: error: value hosts is not a member of Row\n              val hosts = access_logs.map(log =&gt; log.hosts)\n                                                     ^\n</div>","error":null,"workflows":[],"startTime":1.475254861866E12,"submitTime":1.475254861376E12,"finishTime":1.475254862479E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"d471d2f9-284c-48c6-929d-a8541a8d0a60"},{"version":"CommandV1","origId":137283740762873,"guid":"1e41a69a-6184-4b63-b102-55afbe141794","subtype":"command","commandType":"auto","position":9.0,"command":"val dayToHostPairTuple = access_logs.map(log => (log.date_time.day,log.host))\nval dayGroupedHosts = dayToHostPairTuple.groupByKey()\nval dayHostCount = dayGroupedHosts.map(x => (x._1,x._2.toSet.count(x=>true)))\nval dailyHosts = dayHostCount.sortBy(log => log._1).cache()\nval dailyHostsList = dailyHosts.take(30)\n\nprint(\"Unique hosts per day: \")\ndailyHostsList.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Unique hosts per day: (1,2582) (3,3222) (4,4190) (5,2502) (6,2537) (7,4106) (8,4406) (9,4317) (10,4523) (11,2388) dayToHostPairTuple: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[568] at map at &lt;console&gt;:44\ndayGroupedHosts: org.apache.spark.rdd.RDD[(Int, Iterable[String])] = ShuffledRDD[569] at groupByKey at &lt;console&gt;:45\ndayHostCount: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[570] at map at &lt;console&gt;:46\ndailyHosts: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[575] at sortBy at &lt;console&gt;:47\ndailyHostsList: Array[(Int, Int)] = Array((1,2582), (3,3222), (4,4190), (5,2502), (6,2537), (7,4106), (8,4406), (9,4317), (10,4523), (11,2388))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:47: error: missing arguments for method sortByKey in class OrderedRDDFunctions;\nfollow this method with `_' if you want to treat it as a partially applied function\n              val dailyHosts = dayHostCount.sortByKey.cache()\n                                            ^\n</div>","error":null,"workflows":[],"startTime":1.475258808655E12,"submitTime":1.475258808072E12,"finishTime":1.475258809634E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"7b14f1db-8074-4cbb-b2d7-043a6db0bc19"},{"version":"CommandV1","origId":137283740762874,"guid":"2abcdd8c-e8e9-478a-a79c-3be42e628cc2","subtype":"command","commandType":"auto","position":10.0,"command":"// Next, let's determine the average number of requests on a day-by-day basis. We'd like a list by increasing day of the month and the associated average number of requests per host for that day. Make sure you cache the resulting RDD avgDailyReqPerHost so that we can reuse it in the next exercise. To compute the average number of requests per host, get the total number of request across all hosts and divide that by the number of unique hosts. Since the log only covers a single month, you can skip checking for the month. Also to keep it simple, when calculating the approximate average use the integer value.\n// TODO: Replace <FILL IN> with appropriate code\n\nval dayAndHostTuple = access_logs.map(log => (log.date_time.day,log.host))\nval groupedByDay =dayToHostPairTuple.groupByKey()\nval sortedByDay = groupedByDay.sortBy(x => x._1)\nval avgDailyReqPerHost = sortedByDay.map(x=>(x._1,x._2.toList.length/x._2.toSet.count(x=>true))).cache\nval avgDailyReqPerHostList = avgDailyReqPerHost.take(30)\n\nprint(\"Average number of daily requests per Hosts is \")\navgDailyReqPerHostList.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Average number of daily requests per Hosts is (1,13) (3,12) (4,14) (5,12) (6,12) (7,13) (8,13) (9,14) (10,13) (11,13) dayAndHostTuple: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[860] at map at &lt;console&gt;:47\ngroupedByDay: org.apache.spark.rdd.RDD[(Int, Iterable[String])] = ShuffledRDD[861] at groupByKey at &lt;console&gt;:48\nsortedByDay: org.apache.spark.rdd.RDD[(Int, Iterable[String])] = MapPartitionsRDD[866] at sortBy at &lt;console&gt;:49\navgDailyReqPerHost: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[867] at map at &lt;console&gt;:50\navgDailyReqPerHostList: Array[(Int, Int)] = Array((1,13), (3,12), (4,14), (5,12), (6,12), (7,13), (8,13), (9,14), (10,13), (11,13))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:50: error: value length is not a member of scala.collection.immutable.Set[B]\n              val avgDailyReqPerHost = sortedByDay.map(x=&gt;(x._1,x._2.toList.length/x._2.toSet.length))\n                                                                                              ^\n&lt;console&gt;:56: error: value foreach is not a member of Array[Nothing]\n              avgDailyReqPerHostList.foreach(x =&gt; print(x + &quot; &quot;))\n                                     ^\n</div>","error":null,"workflows":[],"startTime":1.475261673001E12,"submitTime":1.475261672347E12,"finishTime":1.475261674442E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"62917b10-2cd6-4e5b-aa55-76bbe13aa86b"},{"version":"CommandV1","origId":137283740762875,"guid":"fee65048-5010-40da-bf07-ae7e60cb2f51","subtype":"command","commandType":"auto","position":11.0,"command":"\nval badRecords = access_logs.filter(log=> log.response_code ==404).cache()\n\nprintln(\"Found \" + badRecords.count() + \" 404 URLs.\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Found 3137 404 URLs.\nbadRecords: org.apache.spark.rdd.RDD[Row] = MapPartitionsRDD[881] at filter at &lt;console&gt;:34\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.475261737277E12,"submitTime":1.475261736621E12,"finishTime":1.475261737442E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a2fc8eb8-3fed-4090-a212-3fbc40031968"},{"version":"CommandV1","origId":137283740762876,"guid":"af4e34a0-c8f6-44cd-806f-95f2433db1fb","subtype":"command","commandType":"auto","position":12.0,"command":"//Now, let's list the 404 response code records. Using the RDD containing only log records with a 404 response code that you cached in the previous part, print out a list up to 40 distinct endpoints that generate 404 errors - no endpoint should appear more than once in your list.\nval badEndpoints = badRecords.map(log => log.endpoint)\nval badUniqueEndpoints = badEndpoints.distinct\nval badUniqueEndpointsPick40 = badUniqueEndpoints.take(40)\n\nprintln(\"404 URLS: \")\nvar i =1\nbadUniqueEndpointsPick40.foreach(x => {println((i)+\": \"+x );i=i+1})","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">404 URLS: \n1: /PERSONS/NASA-CM.\n2: /shuttle/missions/sts-1/sts-1-mission.html\n3: /history/apollo/sa-1/sa-1-patch-small.gif\n4: /public.win3/winvn\n5: /shuttle/technology/STS_newsref/spacelab.html\n6: /shutttle/missions/sts-70/images/KSC-95EC-1059.jpg\n7: /shuttle/missions/sts-70/images/KSC-95EC-o667.gif&quot;\n8: /%3Aspacelink.msfc.nasa.gov\n9: /history/apollo/sa-1/images/\n10: /:/spacelink.msfc.nasa.gov\n11: /sts-71/visitor/\n12: /history/apollo/sa-10/sa-10-patch-small.gif\n13: /elv/updated.gif\n14: /shuttle/missions/mission.html\n15: /enterprise\n16: /space/pub/gif\n17: /netpro/mlm/index.htm\n18: /shuttle/missions/sts-83/mission-sts-83.html\n19: /shuttle/technology/missions/missions.html\n20: /software/winvn/winvn/html\n21: /news/sci.space.shuttle/archive/sci-space-shuttle-22-apr-1995-40.txt\n22: /shuttle/missions/sts-69/mission-sts-69.htlm\n23: /history/gemini/gemini-12.html\n24: /shuttle/missions/sts-67/images/k95p0383.txt\n25: /wwwicons/red.gif\n26: /shuttle/missions/sts-69/mission_sts-69.htlm\n27: /ksc.shtml\n28: /history/apollo/a-004/a-004-patch-small.gif\n29: /shuttle/html\n30: /www/shuttle/countdown/liftoff.html\n31: /\\\\yahoo.com\n32: /history/apollo/apollo-13/apollo-13.info.html.\n33: /elv/ATLAS_CENTAUR/whisae.htm\n34: /gacts/faq12.html\n35: /shuttle/technology/sts-newsref/srb.html#srb\n36: /software/winvn/userguide/1_2.gif\n37: /shuttle/missions/sts-67/images/k95p0381.gif\n38: /shuttle/countdown/images/INDEX.gif\n39: /~terrig/bookmark.html\n40: /shuttle/news/sci.space.news/2169\nbadEndpoints: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[952] at map at &lt;console&gt;:45\nbadUniqueEndpoints: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[955] at distinct at &lt;console&gt;:46\nbadUniqueEndpointsPick40: Array[String] = Array(/PERSONS/NASA-CM., /shuttle/missions/sts-1/sts-1-mission.html, /history/apollo/sa-1/sa-1-patch-small.gif, /public.win3/winvn, /shuttle/technology/STS_newsref/spacelab.html, /shutttle/missions/sts-70/images/KSC-95EC-1059.jpg, /shuttle/missions/sts-70/images/KSC-95EC-o667.gif&quot;, /%3Aspacelink.msfc.nasa.gov, /history/apollo/sa-1/images/, /:/spacelink.msfc.nasa.gov, /sts-71/visitor/, /history/apollo/sa-10/sa-10-patch-small.gif, /elv/updated.gif, /shuttle/missions/mission.html, /enterprise, /space/pub/gif, /netpro/mlm/index.htm, /shuttle/missions/sts-83/mission-sts-83.html, /shuttle/technology/missions/missions.html, /software/winvn/winvn/html, /news/sci.space.shuttle/archive/sci-space-shuttle-22-apr-1995-40.txt, /shuttle/missions/sts-69/mission-sts-69.htlm, /history/gemini/gemini-12.html, /shuttle/missions/sts-67/images/k95p0383.txt, /wwwicons/red.gif, /shuttle/missions/sts-69/mission_sts-69.htlm, /ksc.shtml, /history/apollo/a-004/a-004-patch-small.gif, /shuttle/html, /www/shuttle/countdown/liftoff.html, /\\\\yahoo.com, /history/apollo/apollo-13/apollo-13.info.html., /elv/ATLAS_CENTAUR/whisae.htm, /gacts/faq12.html, /shuttle/technology/sts-newsref/srb.html#srb, /software/winvn/userguide/1_2.gif, /shuttle/missions/sts-67/images/k95p0381.gif, /shuttle/countdown/images/INDEX.gif, /~terrig/bookmark.html, /shuttle/news/sci.space.news/2169)\ni: Int = 41\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:9: error: ';' expected but ')' found.\n       badUniqueEndpointsPick40.foreach(x =&gt; {println((i)+&quot;: &quot;+x );i=i+1)\n                                                                        ^\n</div>","error":null,"workflows":[],"startTime":1.475262239078E12,"submitTime":1.475262238409E12,"finishTime":1.475262239286E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"080f140a-9841-466e-b0f0-27bc7afe2058"},{"version":"CommandV1","origId":137283740762877,"guid":"c90ae2cb-2263-461f-a9c2-f95d2b06d916","subtype":"command","commandType":"auto","position":13.0,"command":"// Using the RDD containing only log records with a 404 response code that you cached before, print out a list of the top twenty endpoints that generate the most 404 errors. Remember, top endpoints should be in sorted order.\n// In [ ]:\n// TODO: Replace <FILL IN> with appropriate code\n\nobject ErrOrdering extends Ordering[(String, Int)] {\n  def compare(a: (String, Int), b: (String, Int)) = a._2 compare b._2\n}\n\nval badEndpointsCountPairTuple = badRecords.map(log=>(log.endpoint,1))\nval badEndpointsSum = badEndpointsCountPairTuple.reduceByKey(_+_)\nval badEndpointsTop20 = badEndpointsSum.takeOrdered(20)(ErrOrdering.reverse)\n\nprint(\"Top Twenty 404 URLs: \")\nbadEndpointsTop20.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Top Twenty 404 URLs: (/images/nasa-logo.gif,319) (/pub/winvn/readme.txt,257) (/pub/winvn/release.txt,199) (/shuttle/missions/STS-69/mission-STS-69.html,181) (/elv/DELTA/uncons.htm,106) (/images/Nasa-logo.gif,85) (/shuttle/missions/sts-68/ksc-upclose.gif,80) (/history/apollo/sa-1/sa-1-patch-small.gif,79) (/images/crawlerway-logo.gif,63) (/://spacelink.msfc.nasa.gov,55) (/history/apollo/a-001/a-001-patch-small.gif,49) (/shuttle/resources/orbiters/atlantis.gif,39) (/history/apollo/images/little-joe.jpg,36) (/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif,36) (/shuttle/countdown/count69.gif,31) (/images/lf-logo.gif,27) (/history/apollo/sa-5/sa-5-patch-small.gif,24) (/shuttle/resources/orbiters/challenger.gif,24) (/robots.txt,23) (/shuttle/resources/orbiters/discovery.gif,20) defined module ErrOrdering\nbadEndpointsCountPairTuple: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1043] at map at &lt;console&gt;:44\nbadEndpointsSum: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[1044] at reduceByKey at &lt;console&gt;:45\nbadEndpointsTop20: Array[(String, Int)] = Array((/images/nasa-logo.gif,319), (/pub/winvn/readme.txt,257), (/pub/winvn/release.txt,199), (/shuttle/missions/STS-69/mission-STS-69.html,181), (/elv/DELTA/uncons.htm,106), (/images/Nasa-logo.gif,85), (/shuttle/missions/sts-68/ksc-upclose.gif,80), (/history/apollo/sa-1/sa-1-patch-small.gif,79), (/images/crawlerway-logo.gif,63), (/://spacelink.msfc.nasa.gov,55), (/history/apollo/a-001/a-001-patch-small.gif,49), (/shuttle/resources/orbiters/atlantis.gif,39), (/history/apollo/images/little-joe.jpg,36), (/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif,36), (/shuttle/countdown/count69.gif,31), (/images/lf-logo.gif,27), (/history/apollo/sa-5/sa-5-patch-small.gif,24), (/shuttle/resources/orbiters/challenger.gif,24), (/robots.txt,23), (/shuttle/resources/orbiters/discovery.gif,20))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:10: error: identifier expected but '(' found.\n       val badEndpointsCountPairTuple = badRecords.map(log=&gt;log.(endpoint,1))\n                                                                ^\n</div>","error":null,"workflows":[],"startTime":1.475263535134E12,"submitTime":1.475263534429E12,"finishTime":1.475263535428E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"f82008b1-c03b-4ce2-9f45-7a9f993bbe5a"},{"version":"CommandV1","origId":137283740762878,"guid":"fefdc42a-3461-4b5b-8d23-2a8ce0d3703f","subtype":"command","commandType":"auto","position":14.0,"command":"//Instead of looking at the endpoints that generated 404 errors, let's look at the hosts that encountered 404 errors. Using the RDD containing only log records with a 404 response code that you cached before, print out a list of the top twenty-five hosts that generate the most 404 errors.\nval errHostsCountPairTuple = badRecords.map(log=>(log.host,1))\nval errHostsSum = errHostsCountPairTuple.reduceByKey(_+_)\nval errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x => x._2))\n\nprint(\"Top 25 hosts that generated errors: \")\nerrHostsTop25.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Top 25 hosts that generated errors: (maz3.maz.net,39) (nexus.mlckew.edu.au,37) (ts8-1.westwood.ts.ucla.edu,37) (piweba3y.prodigy.com,34) (spica.sci.isas.ac.jp,27) (203.13.168.17,25) (203.13.168.24,25) (www-c4.proxy.aol.com,23) (scooter.pa-x.dec.com,23) (onramp2-9.onr.com,22) (crl5.crl.com,22) (198.40.25.102.sap2.artic.edu,21) (msp1-16.nas.mr.net,20) (gn2.getnet.com,20) (isou24.vilspa.esa.es,19) (tigger.nashscene.com,19) (dial055.mbnet.mb.ca,19) (dialup551.chicago.mci.net,18) (utsi057.utsi.com,17) (cougar.oro.net,17) (quadra_alpha.rollins.edu,17) (ix-atl10-08.ix.netcom.com,16) (micromatix.jagunet.com,16) (www-relay.pa-x.dec.com,14) (redx3.cac.washington.edu,14) errHostsCountPairTuple: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1073] at map at &lt;console&gt;:37\nerrHostsSum: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[1074] at reduceByKey at &lt;console&gt;:38\nerrHostsTop25: Array[(String, Int)] = Array((maz3.maz.net,39), (nexus.mlckew.edu.au,37), (ts8-1.westwood.ts.ucla.edu,37), (piweba3y.prodigy.com,34), (spica.sci.isas.ac.jp,27), (203.13.168.17,25), (203.13.168.24,25), (www-c4.proxy.aol.com,23), (scooter.pa-x.dec.com,23), (onramp2-9.onr.com,22), (crl5.crl.com,22), (198.40.25.102.sap2.artic.edu,21), (msp1-16.nas.mr.net,20), (gn2.getnet.com,20), (isou24.vilspa.esa.es,19), (tigger.nashscene.com,19), (dial055.mbnet.mb.ca,19), (dialup551.chicago.mci.net,18), (utsi057.utsi.com,17), (cougar.oro.net,17), (quadra_alpha.rollins.edu,17), (ix-atl10-08.ix.netcom.com,16), (micromatix.jagunet.com,16), (www-relay.pa-x.dec.com,14), (redx3.cac.washington.edu,14))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:4: error: in XML literal: '=' expected instead of '&gt;'\n       val errHostsSum = errHostsCountPairTuple.reduceByKey(&lt;FILL IN&gt;)\n                                                                    ^\n&lt;console&gt;:4: error: in XML literal: ' or &quot; delimited attribute value or '{' scala-expr '}' expected\n       val errHostsSum = errHostsCountPairTuple.reduceByKey(&lt;FILL IN&gt;)\n                                                                     ^\n&lt;console&gt;:5: error: in XML literal: '=' expected instead of 'e'\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n           ^\n&lt;console&gt;:5: error: in XML literal: ' or &quot; delimited attribute value or '{' scala-expr '}' expected\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n            ^\n&lt;console&gt;:5: error: in XML literal: whitespace expected\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n             ^\n&lt;console&gt;:5: error: in XML literal: ' or &quot; delimited attribute value or '{' scala-expr '}' expected\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n                           ^\n&lt;console&gt;:5: error: in XML literal: whitespace expected\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n                            ^\n&lt;console&gt;:5: error: in XML literal: '=' expected instead of '('\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n                                                  ^\n&lt;console&gt;:5: error: in XML literal: ' or &quot; delimited attribute value or '{' scala-expr '}' expected\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n                                                   ^\n&lt;console&gt;:5: error: in XML literal: whitespace expected\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n                                                    ^\n&lt;console&gt;:5: error: in XML literal: '&gt;' expected instead of ')'\n       val errHostsTop25 = errHostsSum.takeOrdered(25)(Ordering[Int].reverse.on(x =&gt; x._2))\n                                                     ^\n</div>","error":null,"workflows":[],"startTime":1.475263965474E12,"submitTime":1.475263964758E12,"finishTime":1.475263965723E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"fc92bdef-695d-4e15-a9fa-4da61bf7e0f9"},{"version":"CommandV1","origId":137283740762879,"guid":"be6ccf2f-830a-4d2c-be56-b7bcb3693d6a","subtype":"command","commandType":"auto","position":15.0,"command":"// Let's explore the 404 records temporally. Break down the 404 requests by day and get the daily counts sorted by day as a list. Since the log only covers a single month, you can ignore the month in your checks.\n// In [ ]:\n// TODO: Replace <FILL IN> with appropriate code\n\nval errDateCountPairTuple = badRecords.map(log=>(log.date_time.day,1))\nval errDateSum = errDateCountPairTuple.reduceByKey(_+_)\nval errDateSorted = errDateSum.cache()\nval errByDate = errDateSorted.takeOrdered(30)\n\nprint(\"404 Errors by day: \")\nerrByDate.foreach(x => print(x + \" \"))\n\nerrDateSorted.cache()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">404 Errors by day: (1,243) (3,303) (4,346) (5,234) (6,372) (7,532) (8,381) (9,279) (10,314) (11,133) errDateCountPairTuple: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[1109] at map at &lt;console&gt;:40\nerrDateSum: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[1110] at reduceByKey at &lt;console&gt;:41\nerrDateSorted: errDateSum.type = ShuffledRDD[1110] at reduceByKey at &lt;console&gt;:41\nerrByDate: Array[(Int, Int)] = Array((1,243), (3,303), (4,346), (5,234), (6,372), (7,532), (8,381), (9,279), (10,314), (11,133))\nres42: errDateSorted.type = ShuffledRDD[1110] at reduceByKey at &lt;console&gt;:41\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.475264135054E12,"submitTime":1.47526413434E12,"finishTime":1.475264135354E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"77c7dec4-7fdf-44a3-894e-ddb1cb2a2b63"},{"version":"CommandV1","origId":137283740762880,"guid":"e2ef3791-9a0b-4b93-845c-6c8bea72aa40","subtype":"command","commandType":"auto","position":16.0,"command":"// using the RDD errDateSorted you cached before, what are the top five days for 404 response codes and the corresponding counts of 404 response codes?\n// In [ ]:\n// TODO: Replace <FILL IN> with appropriate code\n\nobject DateOrdering extends Ordering[(Int, Int)] {\n  def compare(a: (Int, Int), b: (Int, Int)) = a._2 compare b._2\n}\n\nval topErrDate = errDateSorted.takeOrdered(5)(DateOrdering.reverse)\n\nprint(\"Top Five dates for 404 requests: \")\ntopErrDate.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Top Five dates for 404 requests: (7,532) (8,381) (6,372) (4,346) (10,314) defined module DateOrdering\ntopErrDate: Array[(Int, Int)] = Array((7,532), (8,381), (6,372), (4,346), (10,314))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.475264332089E12,"submitTime":1.475264331365E12,"finishTime":1.475264332294E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"5e54fdf1-01af-4c06-a8c4-af3bcb54946b"},{"version":"CommandV1","origId":137283740762881,"guid":"37912af1-4ef9-4660-ab2b-1f5d37a31f63","subtype":"command","commandType":"auto","position":17.0,"command":"// Using the RDD badRecords you cached before, and by hour of the day and in increasing order, create an RDD containing how many requests had a 404 return code for each hour of the day (midnight starts at 0). Cache the resulting RDD hourRecordsSorted and print that as a list.\n// In [ ]:\n// TODO: Replace <FILL IN> with appropriate code\n\nval hourCountPairTuple = badRecords.map(log=> (log.date_time.hour,1))\nval hourRecordsSum = hourCountPairTuple.reduceByKey(_+_)\nval hourRecordsSorted = hourRecordsSum.sortBy(x=>x._1).cache\nval errHourList = hourRecordsSorted.collect()\n\nprint(\"Top hours for 404 requests: \")\nerrHourList.foreach(x => print(x + \" \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Top hours for 404 requests: (0,101) (1,97) (2,343) (3,197) (4,36) (5,27) (6,45) (7,61) (8,121) (9,91) (10,162) (11,136) (12,239) (13,195) (14,104) (15,164) (16,202) (17,134) (18,130) (19,102) (20,157) (21,105) (22,87) (23,101) hourCountPairTuple: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[1262] at map at &lt;console&gt;:48\nhourRecordsSum: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[1263] at reduceByKey at &lt;console&gt;:49\nhourRecordsSorted: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[1268] at sortBy at &lt;console&gt;:50\nerrHourList: Array[(Int, Int)] = Array((0,101), (1,97), (2,343), (3,197), (4,36), (5,27), (6,45), (7,61), (8,121), (9,91), (10,162), (11,136), (12,239), (13,195), (14,104), (15,164), (16,202), (17,134), (18,130), (19,102), (20,157), (21,105), (22,87), (23,101))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:43: error: not enough arguments for method collect: (pf: PartialFunction[(Int, Int),B])(implicit bf: scala.collection.generic.CanBuildFrom[Array[(Int, Int)],B,That])That.\nUnspecified value parameter pf.\n              val errHourList = hourRecordsSorted.collect()\n                                                         ^\n</div>","error":null,"workflows":[],"startTime":1.475264852163E12,"submitTime":1.475264851431E12,"finishTime":1.475264852436E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"141d304a-0c59-4172-af87-343defb06c49"}],"dashboards":[],"guid":"76bbc5af-9ea1-4b40-a1ab-59ca4f82b8e6","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
